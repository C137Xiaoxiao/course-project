{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lFmpE_Wo5gjQP494Wud-pNVQf9c00e4n","timestamp":1670430245997},{"file_id":"1KYvD5dGmnWbSwJldHl_dK86O-WW9nz4m","timestamp":1670187501294},{"file_id":"1ioyp9OX4H9wa1zO9EvdAr4N7CEMyeMP6","timestamp":1669889176516}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["Due Date: December 7th"],"metadata":{"id":"F3ZF3oYIJ3KW"}},{"cell_type":"markdown","metadata":{"id":"YzptQhdh0pUr"},"source":["# Vaccine Development with Dynamic Programming\n","\n","You are the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n","These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n","\n","After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects. \n","\n","Use a discount factor of 0.996.\n","\n","- 1) Write a policy iteration algorithm to compute the value of this project. Please print the full V vector.\n","\n","- 2 )Write a value iteration algorithm to compute the value of this project. Please print the full V vector."]},{"cell_type":"code","metadata":{"id":"dnAvrShs6ecs"},"source":["import numpy as np\n","class MDP():\n","  def __init__(self):\n","    self.A = [0, 1]\n","    self.S = [0, 1, 2, 3, 4]\n","\n","    P0 = np.array([[0.5, .15, .15, 0, .20], # if not invest\n","                   [0, .5, .0, .25, .25],\n","                   [0, 0, .15, .05, .8],\n","                   [0, 0, 0, 0, 1],\n","                   [0, 0, 0, 0, 1]])\n","\n","    R0 = np.array([0, 0, 0, 10, 0]) # payoff unit millions\n","\n","    P1 = np.array([[0.5, .25, .15, 0, .10], #if invest\n","                   [0, .5, .0, .35, .15],\n","                   [0, 0, .20, .05, .75],\n","                   [0, 0, 0, 0, 1],\n","                   [0, 0, 0, 0, 1]])\n","\n","    R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n","\n","    self.P = [P0, P1]\n","    self.R = [R0, R1]\n","\n","  def step(self, s, a):\n","    s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n","    R = self.R[a][s]\n","    if s_prime == 4:\n","      done = True\n","    else:\n","      done = False\n","    return s_prime, R, done\n","\n","  def simulate(self, s, a, π):\n","    done = False\n","    t = 0\n","    history = []\n","    while not done:\n","      if t > 0:\n","        a = π[s]\n","      s_prime, R, done = self.step(s, a)\n","      history.append((s, a, R))\n","      s = s_prime\n","      t += 1\n","\n","    return history\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can access the transition probability matrices and the reward vector as follows:"],"metadata":{"id":"xgAiJxSnZtkH"}},{"cell_type":"code","metadata":{"id":"5-rfjh_37kmX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670818117747,"user_tz":360,"elapsed":167,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}},"outputId":"3127d855-74b6-4edb-a3c0-7dd86f53febc"},"source":["mdp = MDP()\n","P = mdp.P\n","R = mdp.R\n","\n","\n","s = 2 # current state\n","s_prime = 4  # next state\n","a = 1  # chosen action\n","\n","# Probability of transition from state s (2) to s_prime (4) if action == a (1):\n","print(P[a][s, s_prime])\n","\n","# Reward at state s if action = a\n","print(R[a][s])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.75\n","-0.1\n"]}]},{"cell_type":"markdown","source":["## Function for policy/value iteration (ifknown P,R)"],"metadata":{"id":"FWBtLWbo5F2B"}},{"cell_type":"markdown","source":[" Initial stage "],"metadata":{"id":"OMRvR2MAtsMp"}},{"cell_type":"code","source":["γ = 0.996\n","# init state policy 0-4 stage (5 stages) \n","π = [0,0,0,0,0] \n","Qπ = np.zeros((3, 2))\n","Vπ = [0,0,0,0,0]\n","\n","H = mdp.simulate(s, a, π)\n","H\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cczB0wotrbB","executionInfo":{"status":"ok","timestamp":1670821430282,"user_tz":360,"elapsed":150,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}},"outputId":"0611ee7d-d474-4f40-89d8-bc6110f70672"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(2, 1, -0.1)]"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["def construct_Rπ(R, π, S):\n","  Rπ = np.zeros(len(S))\n","  for s in S:\n","    Rπ[s] = R[π[s]][s]\n","  return Rπ\n","\n","def construct_Pπ(P, π, S):\n","  Pπ = np.zeros((len(S), len(S)))\n","  for s in S:\n","    for s_prime in S:\n","      Pπ[s, s_prime] = P[π[s]][s, s_prime]\n","  return Pπ\n","\n","\n","# # Solution with linear algebra\n","# def policy_evaluation(π):\n","#   Rπ = construct_Rπ(R, π, S)\n","#   Pπ = construct_Pπ(P, π, S)\n","#   I = np.eye(3)\n","#   Vπ = np.linalg.solve(I - γ * Pπ, Rπ)\n","#   return Vπ\n","\n","\n","def policy_evaluation(π, Vπ):\n","  Rπ = construct_Rπ(R, π, mdp.S)\n","  Pπ = construct_Pπ(P, π, mdp.S)\n","  for iteration in range(10000):\n","    Vπ = Rπ + γ * Pπ @ Vπ\n","  return Vπ\n","\n","def policy_improvement(Vπ):\n","  Qπ = np.zeros((5, 2))\n","  π_prime = np.zeros(5, dtype=np.int32)\n","  for s in mdp.S: \n","    for a in mdp.A: \n","      Qπ[s, a] = R[a][s] + γ * P[a][s] @ Vπ \n","\n","  # Greedy updates\n","  for s in mdp.S:\n","    π_prime[s] = np.argmax(Qπ[s, :]) \n","  return π_prime\n"],"metadata":{"id":"fhVfgsu18GiJ","executionInfo":{"status":"ok","timestamp":1670821432208,"user_tz":360,"elapsed":127,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["for iteration in range(100):\n","  Vπ = policy_evaluation(π, Vπ)\n","  π = policy_improvement(Vπ)\n","Vπ\n"],"metadata":{"id":"GQdi0iPO8Iu2","executionInfo":{"status":"ok","timestamp":1670821439253,"user_tz":360,"elapsed":5311,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c59827d-0555-4b0b-f7c2-85e2a1c2c21d"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3.32067538,  6.74501992,  0.58546908, 10.        ,  0.        ])"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## Function for policy/value iteration (if NOT known P,R)\n"],"metadata":{"id":"HbiRSnsB8R5M"}},{"cell_type":"markdown","source":["Initial stage"],"metadata":{"id":"7Xq8Cy6naSMJ"}},{"cell_type":"code","source":["γ = 0.996\n","# init state policy 0-4 stage (5 stages) \n","π = [0,0,0,0,0] \n","\n","qπ = np.zeros((5,2))\n","S = np.zeros((5, 2)) \n","N = np.zeros((5, 2))\n","\n","Qπ = np.zeros((5, 2))\n","Vπ = [0,0,0,0,0]\n"],"metadata":{"id":"iAk4pJpa8TB9","executionInfo":{"status":"ok","timestamp":1670821441206,"user_tz":360,"elapsed":119,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def update(π): \n","    s = 1 \n","    a = π[s]\n","    H = mdp.simulate(s, a, π)\n","    T = len(H)\n","    G = 0\n","    for t in np.arange(T - 1, -1, -1):\n","        s, a, R = H[t]\n","        G = γ * G + R\n","        S[s, a] += G\n","        N[s, a] += 1 \n","        qπ[s, a] = qπ[s, a] + 1 / N[s, a] * (G - qπ[s, a]) \n","        π[s] = np.argmax(qπ[s]) \n","    return π\n","\n","for episodes in range(100):\n","    π = update(π)\n","Vπ"],"metadata":{"id":"qXNrKo2caLa5","executionInfo":{"status":"ok","timestamp":1670821447316,"user_tz":360,"elapsed":112,"user":{"displayName":"Alex Zheng","userId":"07361351524997643890"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e4cf12a-6790-42a8-a3f1-1c4a5df44ab3"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 0]"]},"metadata":{},"execution_count":37}]}]}